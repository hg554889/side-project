from abc import ABC, abstractmethod
from typing import List, Dict
from dataclasses import dataclass
import asyncio
import json
import os
import random
from dotenv import load_dotenv
import google.generativeai as genai
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
from bs4 import BeautifulSoup
import requests
import aiohttp
import time
import hashlib
import re
from config.settings import settings
from config.categories import JOB_CATEGORIES, TECH_KEYWORDS
from config.sites_config import SITES_CONFIG  # Add this import
# ë¡œê±° ì„¤ì •
from utils.logger import setup_logger
logger = setup_logger("base_crawler")

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# Gemini API í‚¤ í™•ì¸
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')
if not GEMINI_API_KEY:
    raise ValueError("GEMINI_API_KEY environment variable is not set")

class BaseCrawler(ABC):
    """ê¸°ë³¸ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤"""
    
    def __init__(self, site_name, site_config):
        self.site_name = site_name
        self.site_config = site_config
        self.logger = setup_logger(f"crawler_{site_name}")
        self.setup_driver()
        
    def setup_driver(self):
        """ì›¹ë“œë¼ì´ë²„ ì´ˆê¸°í™”"""
        try:
            chrome_options = Options()
            chrome_options.add_argument('--headless=new')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_experimental_option('excludeSwitches', ['enable-logging'])
            
            # Selenium Manager will automatically handle the driver
            service = Service()
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            self.driver.set_page_load_timeout(30)
            
            self.logger.info("ì›¹ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.warning(f"Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

    def close_driver(self):
        """ì›¹ë“œë¼ì´ë²„ ì¢…ë£Œ"""
        if hasattr(self, 'driver'):
            self.driver.quit()
            
    @abstractmethod
    async def crawl_with_keyword(self, keyword: str) -> List[Dict]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ í¬ë¡¤ë§ (ì¶”ìƒ ë©”ì„œë“œ)"""
        pass
@dataclass
class CrawlJob:
    """í¬ë¡¤ë§ ì‘ì—… ì •ì˜"""
    site_name: str
    keywords: List[str]
    max_jobs: int
    priority: int = 1
    ai_filters: Dict = None

class GeminiAICrawler(BaseCrawler):
    """Gemini AIê°€ í†µí•©ëœ í¬ë¡¤ëŸ¬"""
    
    def __init__(self, site_name, site_config):
        super().__init__(site_name, site_config)
        self.setup_gemini()
        self.crawl_queue = asyncio.Queue()
        
    def setup_gemini(self):
        """Gemini AI ì„¤ì •"""
        api_key = os.getenv('GEMINI_API_KEY')
        if not api_key:
            self.logger.warning("GEMINI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤. AI ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")  # Changed to self.logger
            self.ai_model = None
            return
            
        try:
            genai.configure(api_key=api_key)
            self.ai_model = genai.GenerativeModel('gemini-2.0-flash-exp')
            self.logger.info("Gemini AI ì´ˆê¸°í™” ì™„ë£Œ")  # Changed to self.logger
        except Exception as e:
            self.logger.error(f"Gemini AI ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")  # Changed to self.logger
            self.ai_model = None
    
    async def smart_crawl(self, base_keyword: str, job_category: str = "ê°œë°œì"):
        """AI ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ í¬ë¡¤ë§"""
        try:
            # 1. AIê°€ ê´€ë ¨ í‚¤ì›Œë“œ ìƒì„±
            enhanced_keywords = await self.generate_smart_keywords(
                base_keyword, job_category
            )
            
            # 2. ê° í‚¤ì›Œë“œë¡œ í¬ë¡¤ë§
            all_jobs = []
            for keyword in enhanced_keywords:
                logger.info(f"í‚¤ì›Œë“œ '{keyword}'ë¡œ í¬ë¡¤ë§ ì¤‘...")
                jobs = await self.crawl_with_keyword(keyword)
                all_jobs.extend(jobs)
                
                # ìš”ì²­ ê°„ ëŒ€ê¸° (ì¤‘ìš”!)
                await asyncio.sleep(random.uniform(3, 5))
            
            # 3. AIë¡œ ì¤‘ë³µ ì œê±° ë° í’ˆì§ˆ í•„í„°ë§
            filtered_jobs = await self.ai_filter_jobs(all_jobs)
            
            logger.info(f"ìŠ¤ë§ˆíŠ¸ í¬ë¡¤ë§ ì™„ë£Œ: {len(filtered_jobs)}ê°œ ì±„ìš©ê³µê³ ")
            return filtered_jobs
            
        except Exception as e:
            logger.error(f"AI í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return []
    
    async def generate_smart_keywords(self, base_keyword: str, category: str) -> List[str]:
        """Geminië¡œ ìŠ¤ë§ˆíŠ¸ í‚¤ì›Œë“œ ìƒì„±"""
        if not self.ai_model:
            return [base_keyword]
            
        try:
            # Add delay between API calls
            await asyncio.sleep(10)  # Wait 10 seconds between calls
            
            prompt = f"""
Generate 3-5 related job search keywords for:
- Base keyword: {base_keyword}
- Category: {category}

Requirements:
1. Use actual Korean job posting terms
2. Include technical skills and job titles
3. Keep it specific and relevant

Format: Return only a JSON array
Example: ["Python", "ë°±ì—”ë“œ ê°œë°œì", "Django"]
"""
        
            response = await asyncio.to_thread(
                self.ai_model.generate_content,
                prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.1,
                    max_output_tokens=50
                )
            )
            
            keywords_text = response.text.strip()
            if keywords_text.startswith('```'):
                keywords_text = keywords_text.split('\n')[1:-1]
                keywords_text = '\n'.join(keywords_text)
        
            keywords = json.loads(keywords_text)
            all_keywords = list(set([base_keyword] + keywords))
        
            logger.info(f"AI í‚¤ì›Œë“œ ìƒì„±: {all_keywords}")
            return all_keywords[:3]  # Limit to 3 keywords to reduce API calls
        
        except Exception as e:
            logger.warning(f"AI í‚¤ì›Œë“œ ìƒì„± ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {e}")
            return [base_keyword]
    
    async def ai_filter_jobs(self, jobs: List[Dict]) -> List[Dict]:
        """Geminië¡œ ì±„ìš©ê³µê³  í’ˆì§ˆ í•„í„°ë§"""
        if not self.ai_model or not jobs:
            return jobs
            
        try:
            # ë°°ì¹˜ ì²˜ë¦¬ë¡œ íš¨ìœ¨ì„± í–¥ìƒ
            batch_size = 5
            filtered_jobs = []
            
            for i in range(0, len(jobs), batch_size):
                batch = jobs[i:i+batch_size]
                batch_results = await self.evaluate_job_batch(batch)
                
                for job, score in zip(batch, batch_results):
                    # ì ìˆ˜ê°€ 70ì  ì´ìƒì¸ ê²ƒë§Œ í¬í•¨
                    if score >= 70:
                        job['ai_quality_score'] = score
                        filtered_jobs.append(job)
                
                # API í˜¸ì¶œ ì œí•œ (ë°°ì¹˜ë‹¹ 3ì´ˆ ëŒ€ê¸°)
                await asyncio.sleep(3)
            
            logger.info(f"AI í’ˆì§ˆ í•„í„°ë§: {len(jobs)} â†’ {len(filtered_jobs)}ê°œ")
            return filtered_jobs
            
        except Exception as e:
            logger.warning(f"AI í•„í„°ë§ ì‹¤íŒ¨, ì›ë³¸ ë°˜í™˜: {e}")
            return jobs
    
    async def evaluate_job_batch(self, jobs: List[Dict]) -> List[float]:
        """ì±„ìš©ê³µê³  ë°°ì¹˜ í’ˆì§ˆ í‰ê°€"""
        try:
            job_summaries = []
            for i, job in enumerate(jobs):
                summary = f"""
{i+1}. íšŒì‚¬: {job.get('company_name', 'Unknown')}
   ì§ë¬´: {job.get('job_title', 'ë¯¸ëª…ì‹œ')}
   ìœ„ì¹˜: {job.get('work_location', 'ë¯¸ëª…ì‹œ')}
   ê¸‰ì—¬: {job.get('salary_range', 'ë¯¸ëª…ì‹œ')}
   í‚¤ì›Œë“œ: {job.get('keywords', [])}
"""
                job_summaries.append(summary)
            
            prompt = f"""
ë‹¤ìŒ ì±„ìš©ê³µê³ ë“¤ì˜ í’ˆì§ˆì„ ê°ê° 0-100ì ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.

{chr(10).join(job_summaries)}

í‰ê°€ ê¸°ì¤€:
- ì •ë³´ì˜ êµ¬ì²´ì„± (30ì ): ì§ë¬´, ìš”êµ¬ì‚¬í•­ì´ ëª…í™•í•œê°€?
- íšŒì‚¬ ì‹ ë¢°ì„± (25ì ): íšŒì‚¬ëª…ì´ ëª…í™•í•˜ê³  ì‹ ë¢°í•  ë§Œí•œê°€?
- ê¸‰ì—¬ ì •ë³´ (25ì ): ê¸‰ì—¬ ì •ë³´ê°€ íˆ¬ëª…í•œê°€?
- ê¸°ìˆ ìŠ¤íƒ ì í•©ì„± (20ì ): ê´€ë ¨ ê¸°ìˆ ìŠ¤íƒì´ ëª…ì‹œë˜ì–´ ìˆëŠ”ê°€?

ì‘ë‹µ í˜•ì‹: JSON ë°°ì—´ë¡œ ìˆ«ìë§Œ
ì˜ˆì‹œ: [85, 72, 90, 65, 78]
"""
            
            response = await asyncio.to_thread(
                self.ai_model.generate_content, 
                prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.1,
                    max_output_tokens=100
                )
            )
            
            scores_text = response.text.strip()
            if scores_text.startswith('```'):
                scores_text = scores_text.split('\n')[1:-1]
                scores_text = '\n'.join(scores_text)
            
            scores = json.loads(scores_text)
            
            # ì ìˆ˜ ê²€ì¦ ë° ì •ê·œí™”
            validated_scores = []
            for score in scores:
                validated_score = min(max(float(score), 0), 100)
                validated_scores.append(validated_score)
            
            return validated_scores
            
        except Exception as e:
            logger.warning(f"ë°°ì¹˜ í‰ê°€ ì‹¤íŒ¨: {e}")
            return [75.0] * len(jobs)  # ê¸°ë³¸ê°’ ë°˜í™˜
    
    async def ai_analyze_trends(self, jobs: List[Dict]) -> Dict:
        """ìˆ˜ì§‘ëœ ì±„ìš©ê³µê³  íŠ¸ë Œë“œ ë¶„ì„"""
        if not self.ai_model or not jobs:
            return {}
        
        try:
            # ë°ì´í„° ìš”ì•½ ì¤€ë¹„
            companies = [job.get('company_name', '') for job in jobs]
            titles = [job.get('job_title', '') for job in jobs]
            keywords = []
            for job in jobs:
                keywords.extend(job.get('keywords', []))
            
            prompt = f"""
ë‹¤ìŒ ì±„ìš©ê³µê³  ë°ì´í„°ë¥¼ ë¶„ì„í•´ì„œ íŠ¸ë Œë“œë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”.

ìˆ˜ì§‘ëœ ê³µê³  ìˆ˜: {len(jobs)}ê°œ
ì£¼ìš” íšŒì‚¬ë“¤: {companies[:10]}
ì£¼ìš” ì§ë¬´ë“¤: {titles[:10]}
ê¸°ìˆ  í‚¤ì›Œë“œë“¤: {keywords[:20]}

ë¶„ì„í•  ë‚´ìš©:
1. ì¸ê¸° ìˆëŠ” ê¸°ìˆ ìŠ¤íƒ TOP 5
2. ì£¼ìš” ì±„ìš© íšŒì‚¬ ìœ í˜•
3. í‰ê·  ê¸‰ì—¬ ìˆ˜ì¤€
4. ì£¼ìš” ê·¼ë¬´ ì§€ì—­
5. í˜„ì¬ ì‹œì¥ íŠ¸ë Œë“œ

ì‘ë‹µ í˜•ì‹: JSON ê°ì²´
{{
  "top_skills": ["skill1", "skill2", ...],
  "company_types": ["ëŒ€ê¸°ì—…", "ìŠ¤íƒ€íŠ¸ì—…", ...],
  "salary_insights": "ë¶„ì„ ë‚´ìš©",
  "locations": ["ì„œìš¸", "ê²½ê¸°", ...],
  "market_trend": "ì „ì²´ì ì¸ ì‹œì¥ íŠ¸ë Œë“œ ìš”ì•½"
}}
"""
            
            response = await asyncio.to_thread(
                self.ai_model.generate_content,
                prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.2,
                    max_output_tokens=500
                )
            )
            
            analysis_text = response.text.strip()
            if analysis_text.startswith('```'):
                analysis_text = analysis_text.split('\n')[1:-1]
                analysis_text = '\n'.join(analysis_text)
            
            analysis = json.loads(analysis_text)
            logger.info("AI íŠ¸ë Œë“œ ë¶„ì„ ì™„ë£Œ")
            return analysis
            
        except Exception as e:
            logger.warning(f"íŠ¸ë Œë“œ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    async def schedule_crawling(self, jobs: List[CrawlJob]):
        """í¬ë¡¤ë§ ì‘ì—… ìŠ¤ì¼€ì¤„ë§"""
        # ìš°ì„ ìˆœìœ„ ì •ë ¬
        jobs.sort(key=lambda x: x.priority, reverse=True)
        
        for job in jobs:
            await self.crawl_queue.put(job)
        
        # ì›Œì»¤ ì‹¤í–‰ (ë™ì‹œì„± ì œí•œ)
        workers = [
            asyncio.create_task(self.crawl_worker()) 
            for _ in range(1)  # GeminiëŠ” ë™ì‹œ ìš”ì²­ ì œí•œì´ ìˆìœ¼ë¯€ë¡œ 1ê°œë§Œ
        ]
        
        await self.crawl_queue.join()
        
        # ì›Œì»¤ ì¢…ë£Œ
        for worker in workers:
            worker.cancel()
    
    async def crawl_worker(self):
        """í¬ë¡¤ë§ ì›Œì»¤"""
        while True:
            try:
                job = await self.crawl_queue.get()
                
                logger.info(f"ğŸš€ í¬ë¡¤ë§ ì‹œì‘: {job.site_name} - {job.keywords}")
                
                results = []
                for keyword in job.keywords:
                    job_results = await self.smart_crawl(keyword, "ê°œë°œì")
                    results.extend(job_results[:job.max_jobs])
                    
                    # ì¤‘ìš”: ìš”ì²­ ê°„ ëŒ€ê¸°
                    await asyncio.sleep(random.uniform(5, 8))
                
                # íŠ¸ë Œë“œ ë¶„ì„
                trends = await self.ai_analyze_trends(results)
                
                logger.info(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {len(results)}ê°œ ìˆ˜ì§‘")
                if trends:
                    logger.info(f"ğŸ“Š íŠ¸ë Œë“œ: {trends.get('market_trend', 'N/A')}")
                
                self.crawl_queue.task_done()
                
            except Exception as e:
                logger.error(f"ì›Œì»¤ ì—ëŸ¬: {e}")
                self.crawl_queue.task_done()

    async def crawl_with_keyword(self, keyword: str) -> List[Dict]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ í¬ë¡¤ë§ êµ¬í˜„"""
        max_retries = 3
        retry_count = 0
        
        while retry_count < max_retries:
            try:
                base_url = self.site_config['base_url']
                search_path = self.site_config['search_path']
                url = f"{base_url}{search_path}?keyword={keyword}"
                
                # requests ëª¨ë“œë¡œ ë¨¼ì € ì‹œë„
                results = await self._crawl_with_requests(url)
                if results:
                    return results
                    
                # selenium ëª¨ë“œë¡œ ì¬ì‹œë„
                if self.driver:
                    results = await self._crawl_with_selenium(url)
                    if results:
                        return results
                        
                retry_count += 1
                if retry_count < max_retries:
                    await asyncio.sleep(random.uniform(2, 5))
                    
            except Exception as e:
                self.logger.error(f"í¬ë¡¤ë§ ì‹œë„ {retry_count + 1} ì‹¤íŒ¨: {e}")
                retry_count += 1
                if retry_count < max_retries:
                    await asyncio.sleep(random.uniform(2, 5))
                    
        self.logger.error(f"í‚¤ì›Œë“œ '{keyword}' í¬ë¡¤ë§ ìµœì¢… ì‹¤íŒ¨")
        return []

    async def _crawl_with_selenium(self, url: str) -> List[Dict]:
        """Seleniumì„ ì‚¬ìš©í•œ í¬ë¡¤ë§"""
        if not self.driver:
            return []
            
        try:
            await asyncio.to_thread(self.driver.get, url)
            await asyncio.sleep(2)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            
            job_list_selector = self.site_config['selectors']['job_list']
            wait = WebDriverWait(self.driver, 10)
            await asyncio.to_thread(
                wait.until,
                EC.presence_of_element_located((By.CSS_SELECTOR, job_list_selector))
            )
            
            page_source = await asyncio.to_thread(lambda: self.driver.page_source)
            soup = BeautifulSoup(page_source, 'html.parser')
            return self._parse_job_items(soup)
            
        except Exception as e:
            self.logger.error(f"Selenium í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            # ë“œë¼ì´ë²„ ì¬ì´ˆê¸°í™”
            self.close_driver()
            self.setup_driver()
            return []

    async def _crawl_with_requests(self, url: str) -> List[Dict]:
        """Requestsë¥¼ ì‚¬ìš©í•œ í¬ë¡¤ë§"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/119.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
                'Cache-Control': 'no-cache',
                'Pragma': 'no-cache',
                'DNT': '1'
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, headers=headers, timeout=30) as response:
                    if response.status == 200:
                        html = await response.text()
                        soup = BeautifulSoup(html, 'html.parser')
                        return self._parse_job_items(soup)
                    else:
                        self.logger.warning(f"HTTP ìš”ì²­ ì‹¤íŒ¨: {response.status}")
                        return []
                        
        except Exception as e:
            self.logger.error(f"Requests í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return []
        
    def _parse_job_items(self, soup: BeautifulSoup) -> List[Dict]:
        """HTMLì—ì„œ ì±„ìš©ê³µê³  íŒŒì‹±"""
        results = []
        job_items = soup.select(self.site_config['selectors']['job_list'])
        
        for item in job_items:
            try:
                job_data = {
                    'job_title': item.select_one(self.site_config['selectors']['title']).text.strip(),
                    'company_name': item.select_one(self.site_config['selectors']['company']).text.strip(),
                    'work_location': item.select_one(self.site_config['selectors']['location']).text.strip(),
                    'url': item.select_one(self.site_config['selectors']['url'])['href'],
                    'salary_range': item.select_one(self.site_config['selectors']['salary']).text.strip(),
                }
                results.append(job_data)
            except (AttributeError, KeyError) as e:
                self.logger.warning(f"í•­ëª© íŒŒì‹± ì‹¤íŒ¨: {e}")
                continue
            
        return results

# ì‚¬ìš© ì˜ˆì‹œ
async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    crawler = None
    try:
        # site_config ê°€ì ¸ì˜¤ê¸°
        site_config = SITES_CONFIG.get('saramin')
        if not site_config:
            raise ValueError("ì‚¬ëŒì¸ ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        crawler = GeminiAICrawler("saramin", site_config)
        
        # í¬ë¡¤ë§ ì‘ì—… ì •ì˜
        jobs = [
            CrawlJob(
                site_name="saramin",
                keywords=["React"],
                max_jobs=20,
                priority=1,
                ai_filters={"min_quality": 75}
            ),
            CrawlJob(
                site_name="saramin",
                keywords=["Python"],
                max_jobs=15,
                priority=2
            )
        ]
        
        # ìŠ¤ì¼€ì¤„ë§ ì‹¤í–‰
        logger.info("ğŸ¯ AI ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ í¬ë¡¤ë§ ì‹œì‘!")
        await crawler.schedule_crawling(jobs)
        
    except Exception as e:
        logger.error(f"ë©”ì¸ ì‹¤í–‰ ì—ëŸ¬: {e}")
    finally:
        if crawler and hasattr(crawler, 'driver'):
            crawler.close_driver()

if __name__ == "__main__":
    asyncio.run(main())