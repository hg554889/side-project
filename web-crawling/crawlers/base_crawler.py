from abc import ABC, abstractmethod
from typing import List, Dict
from dataclasses import dataclass
import asyncio
import json
import os
import random
from dotenv import load_dotenv
import google.generativeai as genai
import google.api_core.exceptions
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
from bs4 import BeautifulSoup
import requests
import aiohttp
import time
import hashlib
import re
from config.settings import settings
from config.categories import JOB_CATEGORIES, TECH_KEYWORDS
from config.sites_config import SITES_CONFIG
from config.categories import JOB_CATEGORIES  # Add this import
from database.mongodb_connector import mongodb_connector
from database.redis_connector import redis_connector
# ë¡œê±° ì„¤ì •
from utils.logger import setup_logger
logger = setup_logger("base_crawler")

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# Gemini API í‚¤ í™•ì¸
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')
if not GEMINI_API_KEY:
    raise ValueError("GEMINI_API_KEY environment variable is not set")

class BaseCrawler(ABC):
    """ê¸°ë³¸ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤"""
    
    def __init__(self, site_name, site_config):
        self.site_name = site_name
        self.site_config = site_config
        self.logger = setup_logger(f"crawler_{site_name}")
        self.setup_driver()
        
    def setup_driver(self):
        """ì›¹ë“œë¼ì´ë²„ ì´ˆê¸°í™”"""
        try:
            chrome_options = Options()
            chrome_options.add_argument('--headless=new')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_experimental_option('excludeSwitches', ['enable-logging'])
            
            # Selenium Manager will automatically handle the driver
            service = Service()
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            self.driver.set_page_load_timeout(30)
            
            self.logger.info("ì›¹ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.warning(f"Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

    def close_driver(self):
        """ì›¹ë“œë¼ì´ë²„ ì¢…ë£Œ"""
        if hasattr(self, 'driver'):
            self.driver.quit()
            
    @abstractmethod
    async def crawl_with_keyword(self, keyword: str) -> List[Dict]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ í¬ë¡¤ë§ (ì¶”ìƒ ë©”ì„œë“œ)"""
        pass
@dataclass
class CrawlJob:
    """í¬ë¡¤ë§ ì‘ì—… ì •ì˜"""
    site_name: str
    keywords: List[str]
    max_jobs: int
    priority: int = 1
    ai_filters: Dict = None

class GeminiAICrawler(BaseCrawler):
    """Gemini AIê°€ í†µí•©ëœ í¬ë¡¤ëŸ¬"""
    
    def __init__(self, site_name, site_config):
        super().__init__(site_name, site_config)
        self.setup_gemini()
        self.keyword_cache = {}
        
    def setup_gemini(self):
        """Gemini AI ì„¤ì •"""
        api_key = os.getenv('GEMINI_API_KEY')
        if not api_key:
            self.logger.warning("GEMINI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤. AI ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")  # Changed to self.logger
            self.ai_model = None
            return
            
        try:
            genai.configure(api_key=api_key)
            self.ai_model = genai.GenerativeModel('gemini-2.0-flash-exp')
            self.logger.info("Gemini AI ì´ˆê¸°í™” ì™„ë£Œ")  # Changed to self.logger
        except Exception as e:
            self.logger.error(f"Gemini AI ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")  # Changed to self.logger
            self.ai_model = None
    
    async def smart_crawl(self, base_keyword: str, job_category: str = "ê°œë°œì"):
        """AI ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ í¬ë¡¤ë§"""
        try:
            # 1. AIê°€ ê´€ë ¨ í‚¤ì›Œë“œ ìƒì„±
            enhanced_keywords = await self.generate_smart_keywords(
                base_keyword, job_category
            )
            
            # 2. ê° í‚¤ì›Œë“œë¡œ í¬ë¡¤ë§
            all_jobs = []
            for keyword in enhanced_keywords:
                logger.info(f"í‚¤ì›Œë“œ '{keyword}'ë¡œ í¬ë¡¤ë§ ì¤‘...")
                jobs = await self.crawl_with_keyword(keyword)
                all_jobs.extend(jobs)
                
                # ìš”ì²­ ê°„ ëŒ€ê¸° (ì¤‘ìš”!)
                await asyncio.sleep(random.uniform(3, 5))
            
            # 3. AIë¡œ ì¤‘ë³µ ì œê±° ë° í’ˆì§ˆ í•„í„°ë§
            filtered_jobs = await self.ai_filter_jobs(all_jobs)
            
            logger.info(f"ìŠ¤ë§ˆíŠ¸ í¬ë¡¤ë§ ì™„ë£Œ: {len(filtered_jobs)}ê°œ ì±„ìš©ê³µê³ ")
            return filtered_jobs
            
        except Exception as e:
            logger.error(f"AI í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return []
    
    async def generate_smart_keywords(self, base_keyword: str, category: str) -> List[str]:
        """Geminië¡œ ìŠ¤ë§ˆíŠ¸ í‚¤ì›Œë“œ ìƒì„±"""
        if base_keyword in self.keyword_cache:
            logger.info(f"ìºì‹œì—ì„œ '{base_keyword}'ì— ëŒ€í•œ í‚¤ì›Œë“œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.")
            return self.keyword_cache[base_keyword]

        if not self.ai_model:
            return [base_keyword]

        max_retries = 5
        base_delay = 5  # seconds
        for i in range(max_retries):
            try:
                # Add delay between API calls
                await asyncio.sleep(10)  # Wait 10 seconds between calls
                
                prompt = f"""
Generate 3-5 related job search keywords for:
- Base keyword: {base_keyword}
- Category: {category}

Requirements:
1. Use actual Korean job posting terms
2. Include technical skills and job titles
3. Keep it specific and relevant

Format: Return only a JSON array
Example: ["Python", "ë°±ì—”ë“œ ê°œë°œì", "Django"]
"""
            
                response = await asyncio.to_thread(
                    self.ai_model.generate_content,
                    prompt,
                    generation_config=genai.types.GenerationConfig(
                        temperature=0.1,
                        max_output_tokens=50
                    )
                )
                
                keywords_text = response.text.strip()
                if keywords_text.startswith('```'):
                    keywords_text = keywords_text.split('\n')[1:-1]
                    keywords_text = '\n'.join(keywords_text)
            
                keywords = json.loads(keywords_text)
                all_keywords = list(set([base_keyword] + keywords))
            
                logger.info(f"AI í‚¤ì›Œë“œ ìƒì„±: {all_keywords}")
                self.keyword_cache[base_keyword] = all_keywords[:3]
                return all_keywords[:3]  # Limit to 3 keywords to reduce API calls

            except json.JSONDecodeError as e:
                logger.warning(f"JSON ë””ì½”ë”© ì‹¤íŒ¨: {e}, ì‘ë‹µ: {keywords_text}")
                return [base_keyword]

            except google.api_core.exceptions.ResourceExhausted as e:
                if i == max_retries - 1:
                    logger.error(f"API call failed after {max_retries} retries: {e}")
                    return [base_keyword]
                
                delay = base_delay * (2 ** i) + random.uniform(0, 1)
                logger.warning(f"Rate limit exceeded. Retrying in {delay:.2f} seconds...")
                await asyncio.sleep(delay)

            except Exception as e:
                logger.warning(f"AI í‚¤ì›Œë“œ ìƒì„± ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {e}")
                return [base_keyword]
    
    async def ai_filter_jobs(self, jobs: List[Dict]) -> List[Dict]:
        """Geminië¡œ ì±„ìš©ê³µê³  í’ˆì§ˆ í•„í„°ë§"""
        if not self.ai_model or not jobs:
            return jobs
            
        try:
            # ë°°ì¹˜ ì²˜ë¦¬ë¡œ íš¨ìœ¨ì„± í–¥ìƒ
            batch_size = 5
            filtered_jobs = []
            
            for i in range(0, len(jobs), batch_size):
                batch = jobs[i:i+batch_size]
                batch_results = await self.evaluate_job_batch(batch)
                
                for job, score in zip(batch, batch_results):
                    # ì ìˆ˜ê°€ 70ì  ì´ìƒì¸ ê²ƒë§Œ í¬í•¨
                    if score >= 70:
                        job['ai_quality_score'] = score
                        filtered_jobs.append(job)
                
                # API í˜¸ì¶œ ì œí•œ (ë°°ì¹˜ë‹¹ 3ì´ˆ ëŒ€ê¸°)
                await asyncio.sleep(3)
            
            logger.info(f"AI í’ˆì§ˆ í•„í„°ë§: {len(jobs)} â†’ {len(filtered_jobs)}ê°œ")
            return filtered_jobs
            
        except Exception as e:
            logger.warning(f"AI í•„í„°ë§ ì‹¤íŒ¨, ì›ë³¸ ë°˜í™˜: {e}")
            return jobs
    
    async def evaluate_job_batch(self, jobs: List[Dict]) -> List[float]:
        """ì±„ìš©ê³µê³  ë°°ì¹˜ í’ˆì§ˆ í‰ê°€"""
        max_retries = 5
        base_delay = 5  # seconds
        for i in range(max_retries):
            try:
                job_summaries = []
                for i, job in enumerate(jobs):
                    summary = f"""
{i+1}. íšŒì‚¬: {job.get('company_name', 'Unknown')}
   ì§ë¬´: {job.get('job_title', 'ë¯¸ëª…ì‹œ')}
   ìœ„ì¹˜: {job.get('work_location', 'ë¯¸ëª…ì‹œ')}
   ê¸‰ì—¬: {job.get('salary_range', 'ë¯¸ëª…ì‹œ')}
   í‚¤ì›Œë“œ: {job.get('keywords', [])}
"""
                    job_summaries.append(summary)
                
                prompt = f"""
ë‹¤ìŒ ì±„ìš©ê³µê³ ë“¤ì˜ í’ˆì§ˆì„ ê°ê° 0-100ì ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.

{chr(10).join(job_summaries)}

í‰ê°€ ê¸°ì¤€:
- ì •ë³´ì˜ êµ¬ì²´ì„± (30ì ): ì§ë¬´, ìš”êµ¬ì‚¬í•­ì´ ëª…í™•í•œê°€?
- íšŒì‚¬ ì‹ ë¢°ì„± (25ì ): íšŒì‚¬ëª…ì´ ëª…í™•í•˜ê³  ì‹ ë¢°í•  ë§Œí•œê°€?
- ê¸‰ì—¬ ì •ë³´ (25ì ): ê¸‰ì—¬ ì •ë³´ê°€ íˆ¬ëª…í•œê°€?
- ê¸°ìˆ ìŠ¤íƒ ì í•©ì„± (20ì ): ê´€ë ¨ ê¸°ìˆ ìŠ¤íƒì´ ëª…ì‹œë˜ì–´ ìˆëŠ”ê°€?

ì‘ë‹µ í˜•ì‹: JSON ë°°ì—´ë¡œ ìˆ«ìë§Œ
ì˜ˆì‹œ: [85, 72, 90, 65, 78]
"""
                
                response = await asyncio.to_thread(
                    self.ai_model.generate_content, 
                    prompt,
                    generation_config=genai.types.GenerationConfig(
                        temperature=0.1,
                        max_output_tokens=100
                    )
                )
                
                scores_text = response.text.strip()
                if scores_text.startswith('```'):
                    scores_text = scores_text.split('\n')[1:-1]
                    scores_text = '\n'.join(scores_text)
                
                scores = json.loads(scores_text)
                
                # ì ìˆ˜ ê²€ì¦ ë° ì •ê·œí™”
                validated_scores = []
                for score in scores:
                    validated_score = min(max(float(score), 0), 100)
                    validated_scores.append(validated_score)
                
                return validated_scores

            except json.JSONDecodeError as e:
                logger.warning(f"JSON ë””ì½”ë”© ì‹¤íŒ¨: {e}, ì›ì‹œ ì‘ë‹µ: {response.text}")
                return [75.0] * len(jobs)

            except google.api_core.exceptions.ResourceExhausted as e:
                if i == max_retries - 1:
                    logger.error(f"API call failed after {max_retries} retries: {e}")
                    return [75.0] * len(jobs)
                
                delay = base_delay * (2 ** i) + random.uniform(0, 1)
                logger.warning(f"Rate limit exceeded. Retrying in {delay:.2f} seconds...")
                await asyncio.sleep(delay)
                
            except Exception as e:
                logger.warning(f"ë°°ì¹˜ í‰ê°€ ì‹¤íŒ¨: {e}")
                return [75.0] * len(jobs)
    

    
    async def schedule_crawling(self, jobs: List[CrawlJob]):
        """í¬ë¡¤ë§ ì‘ì—… ìŠ¤ì¼€ì¤„ë§"""
        # ìš°ì„ ìˆœìœ„ ì •ë ¬
        jobs.sort(key=lambda x: x.priority, reverse=True)
        
        for job in jobs:
            # Redis íì— ì‘ì—… ì¶”ê°€
            redis_connector.add_to_queue("crawl_jobs", json.dumps(job.__dict__))
        
        # ì›Œì»¤ ì‹¤í–‰ (ë™ì‹œì„± ì œí•œ)
        workers = [
            asyncio.create_task(self.crawl_worker()) 
            for _ in range(1)  # GeminiëŠ” ë™ì‹œ ìš”ì²­ ì œí•œì´ ìˆìœ¼ë¯€ë¡œ 1ê°œë§Œ
        ]
        
        # Redis íëŠ” join()ì´ í•„ìš” ì—†ìœ¼ë¯€ë¡œ ì œê±°
        
        return workers
    
    async def crawl_worker(self):
        """í¬ë¡¤ë§ ì›Œì»¤"""
        while True:
            try:
                # Redis íì—ì„œ ì‘ì—… ê°€ì ¸ì˜¤ê¸°
                job_str = await asyncio.to_thread(redis_connector.get_from_queue, "crawl_jobs")
                if job_str is None:
                    await asyncio.sleep(1) # íê°€ ë¹„ì–´ìˆìœ¼ë©´ ì ì‹œ ëŒ€ê¸°
                    continue
                
                job = CrawlJob(**json.loads(job_str))
                
                logger.info(f"ğŸš€ í¬ë¡¤ë§ ì‹œì‘: {job.site_name} - {job.keywords}")
                
                all_job_results = []
                for keyword in job.keywords:
                    job_results = await self.smart_crawl(keyword, "ê°œë°œì")
                    all_job_results.extend(job_results[:job.max_jobs])
                    
                    # ì¤‘ìš”: ìš”ì²­ ê°„ ëŒ€ê¸°
                    await asyncio.sleep(random.uniform(5, 8))
                
                # Save results to database
                if all_job_results:
                    logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ì— {len(all_job_results)}ê°œì˜ ì±„ìš©ê³µê³ ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.")
                    for job_posting in all_job_results:
                        await mongodb_connector.insert_job_posting(job_posting)

                logger.info(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {len(all_job_results)}ê°œ ìˆ˜ì§‘")
                # íŠ¸ë Œë“œ ë¶„ì„ ê¸°ëŠ¥ì€ ë³„ë„ ëª¨ë“ˆë¡œ ë¶„ë¦¬ë¨
                
                # Redis íëŠ” task_done()ì´ í•„ìš” ì—†ìŒ

            except asyncio.CancelledError:
                logger.info("ì›Œì»¤ê°€ ì¢…ë£Œë©ë‹ˆë‹¤.")
                break

            except Exception as e:
                logger.error(f"ì›Œì»¤ ì—ëŸ¬: {e}")
                # Redis íëŠ” task_done()ì´ í•„ìš” ì—†ìŒ

    async def crawl_with_keyword(self, keyword: str) -> List[Dict]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ í¬ë¡¤ë§ êµ¬í˜„"""
        max_retries = 3
        retry_count = 0
        
        while retry_count < max_retries:
            try:
                base_url = self.site_config['base_url']
                search_path = self.site_config['search_path']
                url = f"{base_url}{search_path}?keyword={keyword}"
                
                # requests ëª¨ë“œë¡œ ë¨¼ì € ì‹œë„
                results = await self._crawl_with_requests(url)
                if results:
                    return results
                    
                # selenium ëª¨ë“œë¡œ ì¬ì‹œë„
                if self.driver:
                    results = await self._crawl_with_selenium(url)
                    if results:
                        return results
                        
                retry_count += 1
                if retry_count < max_retries:
                    await asyncio.sleep(random.uniform(2, 5))
                    
            except Exception as e:
                self.logger.error(f"í¬ë¡¤ë§ ì‹œë„ {retry_count + 1} ì‹¤íŒ¨: {e}")
                retry_count += 1
                if retry_count < max_retries:
                    await asyncio.sleep(random.uniform(2, 5))
                    
        self.logger.error(f"í‚¤ì›Œë“œ '{keyword}' í¬ë¡¤ë§ ìµœì¢… ì‹¤íŒ¨")
        return []

    def selenium_operations(self, url):
        self.logger.info(f"Seleniumìœ¼ë¡œ URLì— ì ‘ê·¼ ì¤‘: {url}")
        self.driver.get(url)
        time.sleep(2)
        self.logger.info(f"í˜ì´ì§€ íƒ€ì´í‹€: {self.driver.title}")
        job_list_selector = self.site_config['selectors']['job_list']
        self.logger.info(f"'{job_list_selector}' ì„ íƒìë¥¼ ê¸°ë‹¤ë¦¬ëŠ” ì¤‘...")
        wait = WebDriverWait(self.driver, 10)
        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, job_list_selector)))
        self.logger.info("ì„ íƒìë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. í˜ì´ì§€ ì†ŒìŠ¤ë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤.")
        return self.driver.page_source

    async def _crawl_with_selenium(self, url: str) -> List[Dict]:
        """Seleniumì„ ì‚¬ìš©í•œ í¬ë¡¤ë§"""
        if redis_connector.is_url_visited("visited_urls", url):
            self.logger.info(f"ì´ë¯¸ ë°©ë¬¸í•œ URL: {url}")
            return []

        if not self.driver:
            return []
            
        try:
            page_source = await asyncio.to_thread(self.selenium_operations, url)
            soup = BeautifulSoup(page_source, 'html.parser')
            redis_connector.add_visited_url("visited_urls", url) # Add to visited URLs
            return self._parse_job_items(soup)
            
        except Exception as e:
            self.logger.error(f"Selenium í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            
            # ë“œë¼ì´ë²„ ì¬ì´ˆê¸°í™”
            self.close_driver()
            self.setup_driver()
            return []

    async def _crawl_with_requests(self, url: str) -> List[Dict]:
        """Requestsë¥¼ ì‚¬ìš©í•œ í¬ë¡¤ë§"""
        if redis_connector.is_url_visited("visited_urls", url):
            self.logger.info(f"ì´ë¯¸ ë°©ë¬¸í•œ URL: {url}")
            return []

        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/119.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
                'Cache-Control': 'no-cache',
                'Pragma': 'no-cache',
                'DNT': '1'
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, headers=headers, timeout=30) as response:
                    if response.status == 200:
                        html = await response.text()
                        soup = BeautifulSoup(html, 'html.parser')
                        redis_connector.add_visited_url("visited_urls", url) # Add to visited URLs
                        return self._parse_job_items(soup)
                    else:
                        self.logger.warning(f"HTTP ìš”ì²­ ì‹¤íŒ¨: {response.status}")
                        return []
                        
        except Exception as e:
            self.logger.error(f"Requests í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return []
        
    def _parse_job_items(self, soup: BeautifulSoup) -> List[Dict]:
        """HTMLì—ì„œ ì±„ìš©ê³µê³  íŒŒì‹±"""
        results = []
        job_items = soup.select(self.site_config['selectors']['job_list'])
        
        for item in job_items:
            try:
                job_data = {
                    'job_title': item.select_one(self.site_config['selectors']['title']).text.strip(),
                    'company_name': item.select_one(self.site_config['selectors']['company']).text.strip(),
                    'work_location': item.select_one(self.site_config['selectors']['location']).text.strip(),
                    'url': item.select_one(self.site_config['selectors']['url'])['href'],
                    'salary_range': item.select_one(self.site_config['selectors']['salary']).text.strip(),
                }
                results.append(job_data)
            except (AttributeError, KeyError) as e:
                self.logger.warning(f"í•­ëª© íŒŒì‹± ì‹¤íŒ¨: {e}")
                continue
            
        return results

# ì‚¬ìš© ì˜ˆì‹œ
async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    crawler = None
    workers = []
    try:
        await mongodb_connector.connect()
        redis_connector.connect()

        all_workers = []
        target_sites = ["saramin", "jobkorea", "worknet", "comento", "securityfarm"]
        
        # JOB_CATEGORIESì—ì„œ ëª¨ë“  í‚¤ì›Œë“œë¥¼ ë™ì ìœ¼ë¡œ ìˆ˜ì§‘
        all_keywords_from_categories = []
        for category_keywords in JOB_CATEGORIES.values():
            all_keywords_from_categories.extend(category_keywords)
        common_keywords = list(set(all_keywords_from_categories)) # ì¤‘ë³µ ì œê±°

        logger.info(f"--- í¬ë¡¤ë§ ì‹œì‘ (ëŒ€ìƒ ì‚¬ì´íŠ¸: {', '.join(target_sites)}) ---")

        for site_name in target_sites:
            site_config = SITES_CONFIG.get(site_name)
            if not site_config:
                logger.warning(f"{site_name} ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ ì‚¬ì´íŠ¸ëŠ” ê±´ë„ˆëœë‹ˆë‹¤.")
                continue
            
            logger.info(f"--- {site_name} ì‚¬ì´íŠ¸ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ---")
            crawler_instance = GeminiAICrawler(site_name, site_config)
            
            jobs_for_site = [
                CrawlJob(
                    site_name=site_name,
                    keywords=common_keywords,
                    max_jobs=15,
                    priority=site_config.get('priority', 1)
                )
            ]
            
            # ê° ì‚¬ì´íŠ¸ë³„ë¡œ ìŠ¤ì¼€ì¤„ë§í•˜ê³  ì›Œì»¤ë¥¼ ìˆ˜ì§‘
            site_workers = await crawler_instance.schedule_crawling(jobs_for_site)
            all_workers.extend(site_workers)
        
        if not all_workers:
            logger.error("í¬ë¡¤ë§í•  ì‚¬ì´íŠ¸ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ì›Œì»¤ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return

        # ëª¨ë“  ì›Œì»¤ê°€ ì‘ì—…ì„ ì²˜ë¦¬í•  ì‹œê°„ì„ ì£¼ê¸° ìœ„í•´ ì ì‹œ ëŒ€ê¸°
        # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” íê°€ ë¹„ì›Œì§ˆ ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ëŠ” ë¡œì§ì´ í•„ìš”í•©ë‹ˆë‹¤.
        await asyncio.sleep(120) # ëª¨ë“  ì‚¬ì´íŠ¸ì™€ í‚¤ì›Œë“œë¥¼ ê³ ë ¤í•˜ì—¬ ëŒ€ê¸° ì‹œê°„ ì¦ê°€
        
        # finally ë¸”ë¡ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë³€ìˆ˜ ì´ë¦„ í†µì¼
        workers = all_workers

    except Exception as e:
        logger.error(f"ë©”ì¸ ì‹¤í–‰ ì—ëŸ¬: {e}")
    finally:
        # ì›Œì»¤ ì¢…ë£Œ
        for worker in workers:
            worker.cancel()
        if workers:
            await asyncio.gather(*workers, return_exceptions=True)

        if crawler and hasattr(crawler, 'driver'):
            crawler.close_driver()
        
        await mongodb_connector.close()
        redis_connector.close()

if __name__ == "__main__":
    asyncio.run(main())