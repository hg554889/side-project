# ===========================================
# web-crawling/config/settings.py
# ===========================================
import os
from dotenv import load_dotenv

load_dotenv()

class Settings:
    # MongoDB ì„¤ì •
    MONGODB_URI = os.getenv('MONGODB_URI', 'mongodb://localhost:27017/skillmap')
    MONGODB_DB_NAME = 'skillmap'
    
    # Redis ì„¤ì •
    REDIS_HOST = os.getenv('REDIS_HOST', 'localhost')
    REDIS_PORT = int(os.getenv('REDIS_PORT', 6379))
    REDIS_PASSWORD = os.getenv('REDIS_PASSWORD', '')
    
    # í¬ë¡¤ë§ ì„¤ì •
    CRAWL_DELAY = int(os.getenv('CRAWL_DELAY', 3))  # ì´ˆ
    MAX_RETRIES = int(os.getenv('MAX_RETRIES', 3))
    REQUEST_TIMEOUT = int(os.getenv('REQUEST_TIMEOUT', 30))
    HEADLESS_BROWSER = os.getenv('HEADLESS_BROWSER', 'true').lower() == 'true'
    
    # í’ˆì§ˆ ê´€ë¦¬
    MIN_QUALITY_SCORE = float(os.getenv('MIN_QUALITY_SCORE', 0.5))
    MAX_SIMILARITY_SCORE = float(os.getenv('MAX_SIMILARITY_SCORE', 0.8))
    
    # User Agents
    USER_AGENTS = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    ]

settings = Settings()

# ===========================================
# web-crawling/config/sites_config.py
# ===========================================
SITES_CONFIG = {
    'saramin': {
        'base_url': 'https://www.saramin.co.kr',
        'search_path': '/zf_user/jobs/list/domestic',
        'selectors': {
            'job_list': '.item_recruit',
            'title': '.job_tit a',
            'company': '.corp_name a',
            'location': '.job_condition span:first-child',
            'experience': '.job_condition span:nth-child(2)',
            'salary': '.job_condition span:last-child',
            'deadline': '.job_date .date',
            'url': '.job_tit a',
            'tags': '.job_sector a'
        },
        'rate_limit': 3,
        'max_pages': 10
    },
    'comento': {
        'base_url': 'https://comento.kr',
        'search_path': '/career/dreamverse',
        'selectors': {
            'job_list': '[data-testid="job-card"]',
            'title': '.job-title',
            'company': '.company-name',
            'location': '.location',
            'experience': '.experience',
            'skills': '.skill-tag',
            'url': 'a'
        },
        'rate_limit': 4,
        'max_pages': 5
    },
    'jobkorea': {
        'base_url': 'https://www.jobkorea.co.kr',
        'search_path': '/recruit/joblist',
        'selectors': {
            'job_list': '.recruit-info',
            'title': '.post-list-corp-name a',
            'company': '.post-list-info .corp-name a',
            'conditions': '.post-list-info .option',
            'url': '.post-list-corp-name a'
        },
        'rate_limit': 3.5,
        'max_pages': 8
    },
    'securityfarm': {
        'base_url': 'https://securityfarm.co.kr',
        'search_path': '/job',
        'selectors': {
            'job_list': '.job-item',
            'title': '.job-title',
            'company': '.company-name',
            'location': '.location',
            'experience': '.experience',
            'skills': '.skill',
            'deadline': '.deadline',
            'url': 'a'
        },
        'rate_limit': 2,
        'max_pages': 5
    }
}

# ===========================================
# web-crawling/config/categories.py
# ===========================================
JOB_CATEGORIES = {
    'IT/ê°œë°œ': [
        'developer', 'ê°œë°œì', 'programmer', 'í”„ë¡œê·¸ë˜ë¨¸',
        'frontend', 'backend', 'fullstack', 'í”„ë¡ íŠ¸ì—”ë“œ', 'ë°±ì—”ë“œ', 'í’€ìŠ¤íƒ',
        'react', 'vue', 'angular', 'node.js', 'spring', 'django', 'python', 'java'
    ],
    'ë³´ì•ˆ': [
        'ë³´ì•ˆ', 'security', 'ì¹¨í•´ëŒ€ì‘', 'ë³´ì•ˆê´€ì œ', 'soc', 'ì·¨ì•½ì ',
        'í¬ë Œì‹', 'ì •ë³´ë³´ì•ˆ', 'ë„¤íŠ¸ì›Œí¬ë³´ì•ˆ', 'ì¸í”„ë¼ë³´ì•ˆ'
    ],
    'ë§ˆì¼€íŒ…': [
        'ë§ˆì¼€íŒ…', 'marketing', 'ê´‘ê³ ', 'ë¸Œëœë“œ', 'í¼í¬ë¨¼ìŠ¤',
        'seo', 'sem', 'ì½˜í…ì¸ ', 'ë””ì§€í„¸ë§ˆì¼€íŒ…', 'ì˜¨ë¼ì¸ë§ˆì¼€íŒ…'
    ],
    'ë””ìì¸': [
        'ë””ìì¸', 'design', 'ui', 'ux', 'ê·¸ë˜í”½',
        'figma', 'adobe', 'photoshop', 'illustrator', 'ì›¹ë””ìì¸'
    ],
    'ê¸°íš': [
        'ê¸°íš', 'pm', 'product manager', 'ì„œë¹„ìŠ¤ê¸°íš',
        'ì „ëµê¸°íš', 'ì‚¬ì—…ê¸°íš', 'planning', 'í”„ë¡œë•íŠ¸'
    ],
    'ì˜ì—…/ì„¸ì¼ì¦ˆ': [
        'ì˜ì—…', 'ì„¸ì¼ì¦ˆ', 'sales', 'ê¸°ìˆ ì˜ì—…',
        'b2b', 'b2c', 'í•´ì™¸ì˜ì—…', 'êµ­ë‚´ì˜ì—…'
    ],
    'ê¸ˆìœµ': [
        'ê¸ˆìœµ', 'finance', 'íˆ¬ì', 'ë¦¬ìŠ¤í¬', 'íšŒê³„',
        'ì¬ë¬´', 'cpa', 'frm', 'ì€í–‰', 'ì¦ê¶Œ'
    ]
}

TECH_KEYWORDS = [
    'JavaScript', 'TypeScript', 'Python', 'Java', 'C++', 'C#', 'Go', 'Rust',
    'React', 'Vue', 'Angular', 'Node.js', 'Express', 'Spring', 'Django', 'Flask',
    'AWS', 'Azure', 'GCP', 'Docker', 'Kubernetes', 'Jenkins',
    'MongoDB', 'MySQL', 'PostgreSQL', 'Redis', 'Elasticsearch',
    'Git', 'Jira', 'Figma', 'Adobe', 'Photoshop', 'Illustrator',
    'HTML', 'CSS', 'SQL', 'NoSQL', 'REST', 'GraphQL'
]

# ===========================================
# web-crawling/utils/logger.py  
# ===========================================
from loguru import logger
import sys
from pathlib import Path

def setup_logger():
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    log_dir = Path(__dirname__).parent / 'logs'
    log_dir.mkdir(exist_ok=True)
    
    # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°
    logger.remove()
    
    # ì½˜ì†” ë¡œê·¸
    logger.add(
        sys.stdout,
        format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>",
        level="INFO"
    )
    
    # íŒŒì¼ ë¡œê·¸
    logger.add(
        log_dir / "crawler.log",
        format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} - {message}",
        level="DEBUG",
        rotation="10 MB",
        retention="30 days"
    )
    
    return logger

# ===========================================
# web-crawling/database/mongo_client.py
# ===========================================
from pymongo import MongoClient
from pymongo.errors import ConnectionFailure
from config.settings import settings
from utils.logger import setup_logger

logger = setup_logger()

class MongoDBClient:
    _instance = None
    _client = None
    _db = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def connect(self):
        if self._client is None:
            try:
                self._client = MongoClient(settings.MONGODB_URI)
                self._db = self._client[settings.MONGODB_DB_NAME]
                # ì—°ê²° í…ŒìŠ¤íŠ¸
                self._client.admin.command('ping')
                logger.info(f"MongoDB ì—°ê²° ì„±ê³µ: {settings.MONGODB_DB_NAME}")
            except ConnectionFailure as e:
                logger.error(f"MongoDB ì—°ê²° ì‹¤íŒ¨: {e}")
                raise
        return self._db
    
    def get_collection(self, collection_name):
        db = self.connect()
        return db[collection_name]
    
    def close(self):
        if self._client:
            self._client.close()
            self._client = None
            self._db = None
            logger.info("MongoDB ì—°ê²° ì¢…ë£Œ")

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
mongo_client = MongoDBClient()

# ===========================================
# web-crawling/crawlers/base_crawler.py
# ===========================================
from abc import ABC, abstractmethod
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
from bs4 import BeautifulSoup
import requests
import time
import hashlib
import re
import random
from config.settings import settings
from config.categories import JOB_CATEGORIES, TECH_KEYWORDS
from utils.logger import setup_logger

logger = setup_logger()

class BaseCrawler(ABC):
    def __init__(self, site_name, site_config):
        self.site_name = site_name
        self.config = site_config
        self.base_url = site_config['base_url']
        self.selectors = site_config['selectors']
        self.rate_limit = site_config.get('rate_limit', 3)
        self.driver = None
        
    def setup_driver(self):
        """Chrome ë“œë¼ì´ë²„ ì„¤ì •"""
        options = Options()
        options.add_argument(f'--user-agent={random.choice(settings.USER_AGENTS)}')
        
        if settings.HEADLESS_BROWSER:
            options.add_argument('--headless')
        
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--window-size=1920,1080')
        
        try:
            self.driver = webdriver.Chrome(options=options)
            self.driver.set_page_load_timeout(settings.REQUEST_TIMEOUT)
            logger.info(f"{self.site_name}: Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì™„ë£Œ")
        except WebDriverException as e:
            logger.error(f"{self.site_name}: ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨ - {e}")
            raise
    
    def close_driver(self):
        """ë“œë¼ì´ë²„ ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()
            self.driver = None
            logger.info(f"{self.site_name}: ë“œë¼ì´ë²„ ì¢…ë£Œ")
    
    @abstractmethod
    async def crawl(self, options=None):
        """í¬ë¡¤ë§ ë©”ì¸ ë©”ì„œë“œ - í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„"""
        pass
    
    @abstractmethod
    def extract_job_data(self, element):
        """ì±„ìš©ê³µê³  ë°ì´í„° ì¶”ì¶œ - í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„"""
        pass
    
    def normalize_data(self, raw_data):
        """ë°ì´í„° ì •ê·œí™”"""
        return {
            'id': self.generate_id(raw_data.get('url', '')),
            'source': self.site_name,
            'url': raw_data.get('url', ''),
            
            'company_name': raw_data.get('company', '').strip(),
            'company_location': self.parse_location(raw_data.get('location', '')),
            
            'job_title': raw_data.get('title', '').strip(),
            'job_category': self.categorize_job(raw_data.get('title', ''), raw_data.get('tags', [])),
            'experience_level': self.normalize_experience(raw_data.get('experience', '')),
            
            'work_location': raw_data.get('location', '').strip(),
            'salary_range': self.parse_salary(raw_data.get('salary', '')),
            
            'keywords': self.extract_keywords(raw_data.get('title', ''), raw_data.get('tags', [])),
            'deadline': self.parse_date(raw_data.get('deadline', '')),
            'scraped_at': time.time(),
            'is_active': True
        }
    
    def generate_id(self, url):
        """URL ê¸°ë°˜ ê³ ìœ  ID ìƒì„±"""
        return hashlib.md5(url.encode()).hexdigest()
    
    def categorize_job(self, title, skills=None):
        """ì§êµ° ë¶„ë¥˜"""
        if skills is None:
            skills = []
            
        text = (title + ' ' + ' '.join(skills)).lower()
        
        for category, keywords in JOB_CATEGORIES.items():
            if any(keyword.lower() in text for keyword in keywords):
                return category
        
        return 'ê¸°íƒ€'
    
    def normalize_experience(self, experience):
        """ê²½í—˜ ìˆ˜ì¤€ ì •ê·œí™”"""
        exp = experience.lower()
        
        if any(word in exp for word in ['ì‹ ì…', 'entry', '0ë…„']):
            return 'ì‹ ì…'
        elif any(word in exp for word in ['1ë…„', '2ë…„', '3ë…„']):
            return '1-3ë…„ì°¨'
        elif 'ë¬´ê´€' in exp or 'any' in exp:
            return 'ê²½ë ¥ë¬´ê´€'
        
        return 'ê²½ë ¥ë¬´ê´€'
    
    def extract_keywords(self, title, skills=None):
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        if skills is None:
            skills = []
            
        text = title + ' ' + ' '.join(skills)
        extracted = []
        
        for keyword in TECH_KEYWORDS:
            if keyword.lower() in text.lower():
                extracted.append(keyword)
        
        # ì¤‘ë³µ ì œê±°
        return list(set(extracted + skills))
    
    def parse_location(self, location_text):
        """ìœ„ì¹˜ ì •ê·œí™”"""
        location = location_text.strip()
        
        location_mapping = {
            'ì„œìš¸ì‹œ': 'ì„œìš¸',
            'ì„œìš¸íŠ¹ë³„ì‹œ': 'ì„œìš¸',
            'ê²½ê¸°ë„': 'ê²½ê¸°',
            'ë¶€ì‚°ì‹œ': 'ë¶€ì‚°',
            'ë¶€ì‚°ê´‘ì—­ì‹œ': 'ë¶€ì‚°',
            'ì¬íƒê·¼ë¬´': 'ì¬íƒ',
            'ì›ê²©ê·¼ë¬´': 'ì¬íƒ'
        }
        
        return location_mapping.get(location, location)
    
    def parse_salary(self, salary_text):
        """ê¸‰ì—¬ ì •ë³´ íŒŒì‹±"""
        if not salary_text:
            return {'min': 0, 'max': 0, 'negotiable': True}
        
        # ìˆ«ì ì¶”ì¶œ
        numbers = re.findall(r'\d+', salary_text)
        if not numbers:
            return {'min': 0, 'max': 0, 'negotiable': True}
        
        nums = [int(n) for n in numbers]
        
        return {
            'min': min(nums) * 10000 if nums else 0,  # ë§Œì› -> ì›
            'max': max(nums) * 10000 if nums else 0,
            'negotiable': any(word in salary_text for word in ['í˜‘ì˜', 'ë©´ì ‘', 'ìƒë‹´'])
        }
    
    def parse_date(self, date_text):
        """ë‚ ì§œ íŒŒì‹±"""
        if not date_text or 'ìƒì‹œ' in date_text:
            return None
        
        if 'ë§ˆê°' in date_text:
            return time.time()
        
        # YYYY-MM-DD í˜•ì‹ ì°¾ê¸°
        date_match = re.search(r'(\d{4})[-.](\d{1,2})[-.](\d{1,2})', date_text)
        if date_match:
            year, month, day = date_match.groups()
            try:
                import datetime
                dt = datetime.datetime(int(year), int(month), int(day))
                return dt.timestamp()
            except ValueError:
                return None
        
        return None
    
    def wait_and_find_element(self, by, value, timeout=10):
        """ìš”ì†Œ ëŒ€ê¸° ë° ì°¾ê¸°"""
        try:
            element = WebDriverWait(self.driver, timeout).until(
                EC.presence_of_element_located((by, value))
            )
            return element
        except TimeoutException:
            logger.warning(f"{self.site_name}: ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ - {value}")
            return None
    
    def scroll_to_load_more(self, max_scrolls=5):
        """í˜ì´ì§€ ìŠ¤í¬ë¡¤í•˜ì—¬ ë” ë§ì€ ì½˜í…ì¸  ë¡œë“œ"""
        for i in range(max_scrolls):
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            
            # ìƒˆë¡œìš´ ì½˜í…ì¸ ê°€ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
            new_height = self.driver.execute_script("return document.body.scrollHeight")
            if i > 0 and new_height == last_height:
                break
            last_height = new_height
    
    def delay(self):
        """ìš”ì²­ ê°„ ëŒ€ê¸°"""
        time.sleep(self.rate_limit + random.uniform(0, 1))

# ===========================================
# web-crawling/crawlers/saramin_crawler.py
# ===========================================
from .base_crawler import BaseCrawler
from selenium.webdriver.common.by import By
from selenium.common.exceptions import NoSuchElementException
from config.sites_config import SITES_CONFIG
from utils.logger import setup_logger
import time

logger = setup_logger()

class SaraminCrawler(BaseCrawler):
    def __init__(self):
        super().__init__('saramin', SITES_CONFIG['saramin'])
    
    async def crawl(self, options=None):
        if options is None:
            options = {}
        
        keyword = options.get('keyword', '')
        category = options.get('category', '')
        experience_level = options.get('experience_level', '')
        max_jobs = options.get('max_jobs', 50)
        
        jobs = []
        
        try:
            self.setup_driver()
            
            # ê²€ìƒ‰ URL êµ¬ì„±
            search_params = {
                'recruitFilterType': 'domestic',
                'searchType': 'search',
                'searchword': keyword,
                'cat_kewd': category,
                'exp_cd': self.map_experience_level(experience_level)
            }
            
            # URL ìƒì„±
            params_str = '&'.join([f"{k}={v}" for k, v in search_params.items() if v])
            url = f"{self.base_url}{self.config['search_path']}?{params_str}"
            
            logger.info(f"ì‚¬ëŒì¸ í¬ë¡¤ë§ ì‹œì‘: {url}")
            
            self.driver.get(url)
            
            # ì±„ìš©ê³µê³  ë¦¬ìŠ¤íŠ¸ ëŒ€ê¸°
            job_list_element = self.wait_and_find_element(By.CSS_SELECTOR, self.selectors['job_list'])
            if not job_list_element:
                logger.warning("ì±„ìš©ê³µê³  ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return jobs
            
            # í˜ì´ì§€ ìŠ¤í¬ë¡¤ (ë” ë§ì€ ê³µê³  ë¡œë“œ)
            self.scroll_to_load_more(3)
            
            # ëª¨ë“  ì±„ìš©ê³µê³  ìš”ì†Œ ì°¾ê¸°
            job_elements = self.driver.find_elements(By.CSS_SELECTOR, self.selectors['job_list'])
            logger.info(f"ì‚¬ëŒì¸: {len(job_elements)}ê°œ ì±„ìš©ê³µê³  ë°œê²¬")
            
            for i, element in enumerate(job_elements[:max_jobs]):
                try:
                    raw_data = self.extract_job_data(element)
                    if raw_data and raw_data.get('title') and raw_data.get('company'):
                        normalized_data = self.normalize_data(raw_data)
                        jobs.append(normalized_data)
                        logger.debug(f"ì‚¬ëŒì¸: {i+1}ë²ˆì§¸ ê³µê³  ì¶”ì¶œ ì™„ë£Œ - {raw_data.get('title')}")
                    
                except Exception as e:
                    logger.warning(f"ì‚¬ëŒì¸: {i+1}ë²ˆì§¸ ê³µê³  ì¶”ì¶œ ì‹¤íŒ¨ - {e}")
                    continue
            
            logger.info(f"ì‚¬ëŒì¸: ì´ {len(jobs)}ê°œ ì±„ìš©ê³µê³  ìˆ˜ì§‘ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ì‚¬ëŒì¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            raise
        
        finally:
            self.close_driver()
            self.delay()
        
        return jobs
    
    def extract_job_data(self, element):
        """ê°œë³„ ì±„ìš©ê³µê³  ë°ì´í„° ì¶”ì¶œ"""
        try:
            # ì œëª©
            title_elem = element.find_element(By.CSS_SELECTOR, self.selectors['title'])
            title = title_elem.text.strip() if title_elem else ''
            url = title_elem.get_attribute('href') if title_elem else ''
            
            # íšŒì‚¬ëª…
            company_elem = element.find_element(By.CSS_SELECTOR, self.selectors['company'])
            company = company_elem.text.strip() if company_elem else ''
            
            # ìœ„ì¹˜, ê²½í—˜, ê¸‰ì—¬
            condition_elements = element.find_elements(By.CSS_SELECTOR, '.job_condition span')
            location = condition_elements[0].text.strip() if len(condition_elements) > 0 else ''
            experience = condition_elements[1].text.strip() if len(condition_elements) > 1 else ''
            salary = condition_elements[2].text.strip() if len(condition_elements) > 2 else ''
            
            # ë§ˆê°ì¼
            try:
                deadline_elem = element.find_element(By.CSS_SELECTOR, self.selectors['deadline'])
                deadline = deadline_elem.text.strip() if deadline_elem else ''
            except NoSuchElementException:
                deadline = ''
            
            # íƒœê·¸/ìŠ¤í‚¬
            try:
                tag_elements = element.find_elements(By.CSS_SELECTOR, self.selectors['tags'])
                tags = [tag.text.strip() for tag in tag_elements if tag.text.strip()]
            except NoSuchElementException:
                tags = []
            
            return {
                'title': title,
                'company': company,
                'location': location,
                'experience': experience,
                'salary': salary,
                'deadline': deadline,
                'url': url,
                'tags': tags
            }
            
        except Exception as e:
            logger.warning(f"ì‚¬ëŒì¸: ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨ - {e}")
            return None
    
    def map_experience_level(self, level):
        """ê²½í—˜ ìˆ˜ì¤€ ë§¤í•‘"""
        mapping = {
            'ì‹ ì…': '1',
            '1-3ë…„ì°¨': '2',
            'ê²½ë ¥ë¬´ê´€': '0'
        }
        return mapping.get(level, '0')

# ===========================================
# web-crawling/processors/data_normalizer.py
# ===========================================
from typing import List, Dict, Any
import re
from utils.logger import setup_logger
from database.mongo_client import mongo_client

logger = setup_logger()

class DataNormalizer:
    def __init__(self):
        self.company_mapper = CompanyNameMapper()
        self.location_normalizer = LocationNormalizer()
        self.skills_normalizer = SkillsNormalizer()
        self.salary_normalizer = SalaryNormalizer()
    
    async def normalize(self, raw_job: Dict[str, Any]) -> Dict[str, Any]:
        """ë°ì´í„° ì •ê·œí™” ë©”ì¸ ë©”ì„œë“œ"""
        normalized = raw_job.copy()
        
        try:
            # 1. íšŒì‚¬ëª… ì •ê·œí™”
            if 'company_name' in normalized:
                normalized['company_name'] = await self.company_mapper.normalize(
                    normalized['company_name']
                )
            
            # 2. ìœ„ì¹˜ ì •ê·œí™”
            if 'work_location' in normalized:
                normalized['work_location'] = self.location_normalizer.normalize(
                    normalized['work_location']
                )
            
            # 3. ìŠ¤í‚¬/í‚¤ì›Œë“œ ì •ê·œí™”
            if 'keywords' in normalized:
                normalized['keywords'] = self.skills_normalizer.normalize(
                    normalized['keywords']
                )
            
            # 4. ê¸‰ì—¬ ì •ê·œí™”
            if 'salary_range' in normalized:
                normalized['salary_range'] = self.salary_normalizer.normalize(
                    normalized['salary_range']
                )
            
            # 5. ì§êµ° ë¶„ë¥˜ ê°œì„ 
            normalized['job_category'] = await self.improve_job_categorization(normalized)
            
            # 6. í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            normalized['quality_score'] = self.calculate_quality_score(normalized)
            
            return normalized
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ì •ê·œí™” ì‹¤íŒ¨: {e}")
            return {**raw_job, 'quality_score': 0.3, 'normalization_error': str(e)}
    
    def calculate_quality_score(self, job: Dict[str, Any]) -> float:
        """ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        score = 0
        max_score = 0
        
        # í•„ìˆ˜ í•„ë“œ ê²€ì‚¬
        required_fields = ['job_title', 'company_name', 'job_category']
        for field in required_fields:
            max_score += 20
            if job.get(field) and str(job[field]).strip():
                score += 20
        
        # ì„ íƒ í•„ë“œ ê²€ì‚¬
        optional_fields = ['work_location', 'keywords', 'salary_range']
        for field in optional_fields:
            max_score += 10
            value = job.get(field)
            if value and (isinstance(value, list) and len(value) > 0 or value):
                score += 10
        
        # í‚¤ì›Œë“œ í’ë¶€ë„
        max_score += 30
        keywords = job.get('keywords', [])
        if isinstance(keywords, list) and len(keywords) >= 3:
            score += 30
        elif isinstance(keywords, list) and len(keywords) >= 1:
            score += 15
        
        return min(score / max_score, 1.0) if max_score > 0 else 0.0
    
    async def improve_job_categorization(self, job: Dict[str, Any]) -> str:
        """AIë¥¼ ì‚¬ìš©í•œ ë” ì •í™•í•œ ì§êµ° ë¶„ë¥˜"""
        context = ' '.join([
            job.get('job_title', ''),
            ' '.join(job.get('keywords', [])),
        ]).strip()
        
        # ê¸°ì¡´ ì¹´í…Œê³ ë¦¬ ì‹ ë¢°ë„ ê²€ì¦
        current_category = job.get('job_category', 'ê¸°íƒ€')
        confidence = self.get_category_confidence(current_category, context)
        
        if confidence < 0.7:
            # ì‹ ë¢°ë„ê°€ ë‚®ìœ¼ë©´ ì¬ë¶„ë¥˜
            return self.reclassify_job(context)
        
        return current_category
    
    def get_category_confidence(self, category: str, context: str) -> float:
        """ì¹´í…Œê³ ë¦¬ ì‹ ë¢°ë„ ê³„ì‚°"""
        from config.categories import JOB_CATEGORIES
        
        keywords = JOB_CATEGORIES.get(category, [])
        if not keywords:
            return 0.0
        
        context_lower = context.lower()
        matches = sum(1 for keyword in keywords if keyword.lower() in context_lower)
        
        return min(matches / len(keywords), 1.0)
    
    def reclassify_job(self, context: str) -> str:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ì¬ë¶„ë¥˜"""
        from config.categories import JOB_CATEGORIES
        
        best_category = 'ê¸°íƒ€'
        max_score = 0
        
        context_lower = context.lower()
        
        for category, keywords in JOB_CATEGORIES.items():
            score = sum(1 for keyword in keywords if keyword.lower() in context_lower)
            if score > max_score:
                max_score = score
                best_category = category
        
        return best_category

class CompanyNameMapper:
    def __init__(self):
        self.aliases = self._load_company_aliases()
    
    def _load_company_aliases(self):
        """íšŒì‚¬ëª… ë³„ì¹­ ë§¤í•‘ ë¡œë“œ"""
        return {
            'ì‚¼ì„±ì „ì': ['ì‚¼ì„±ì „ìì£¼ì‹íšŒì‚¬', 'Samsung Electronics', 'SEC'],
            'ë„¤ì´ë²„': ['NAVER', 'ë„¤ì´ë²„ì£¼ì‹íšŒì‚¬', 'NAVER Corp'],
            'ì¹´ì¹´ì˜¤': ['Kakao', 'ì£¼ì‹íšŒì‚¬ì¹´ì¹´ì˜¤', 'Kakao Corp'],
            'LGì „ì': ['LG Electronics', 'LGì „ìì£¼ì‹íšŒì‚¬'],
            'í˜„ëŒ€ìë™ì°¨': ['í˜„ëŒ€ìë™ì°¨ì£¼ì‹íšŒì‚¬', 'Hyundai Motor'],
        }
    
    async def normalize(self, company_name: str) -> str:
        """íšŒì‚¬ëª… ì •ê·œí™”"""
        if not company_name:
            return ''
        
        # 1. ê¸°ë³¸ ì •ë¦¬
        normalized = company_name.strip()
        normalized = re.sub(r'ì£¼ì‹íšŒì‚¬|ãˆœ|\(ì£¼\)', '', normalized)
        normalized = re.sub(r'\s+', ' ', normalized).strip()
        
        # 2. ë³„ì¹­ ë§¤í•‘
        for canonical, aliases in self.aliases.items():
            if normalized.lower() in [alias.lower() for alias in aliases]:
                return canonical
        
        return normalized

class LocationNormalizer:
    def __init__(self):
        self.location_mapping = {
            'ì„œìš¸ì‹œ': 'ì„œìš¸',
            'ì„œìš¸íŠ¹ë³„ì‹œ': 'ì„œìš¸',
            'ê²½ê¸°ë„': 'ê²½ê¸°',
            'ë¶€ì‚°ì‹œ': 'ë¶€ì‚°',
            'ë¶€ì‚°ê´‘ì—­ì‹œ': 'ë¶€ì‚°',
            'ì¬íƒê·¼ë¬´': 'ì¬íƒ',
            'ì›ê²©ê·¼ë¬´': 'ì¬íƒ',
        }
    
    def normalize(self, location: str) -> str:
        """ìœ„ì¹˜ ì •ê·œí™”"""
        if not location:
            return ''
        
        normalized = location.strip()
        
        # ë§¤í•‘ í…Œì´ë¸” ì ìš©
        mapped = self.location_mapping.get(normalized)
        if mapped:
            return mapped
        
        # íŒ¨í„´ ë§¤ì¹­
        if 'ê°•ë‚¨' in normalized and 'ì„œìš¸' not in normalized:
            return 'ì„œìš¸ ê°•ë‚¨êµ¬'
        
        if 'íŒêµ' in normalized and 'ê²½ê¸°' not in normalized:
            return 'ê²½ê¸° ì„±ë‚¨ íŒêµ'
        
        return re.sub(r'ì‹œ$|êµ¬$|ë™$', '', normalized).strip()

class SkillsNormalizer:
    def __init__(self):
        self.skill_mappings = {
            'js': 'JavaScript',
            'ts': 'TypeScript',
            'python': 'Python',
            'python3': 'Python',
            'react.js': 'React',
            'reactjs': 'React',
            'vue.js': 'Vue.js',
            'vuejs': 'Vue.js',
            'node.js': 'Node.js',
            'nodejs': 'Node.js',
            'aws': 'AWS',
            'mysql': 'MySQL',
            'postgresql': 'PostgreSQL',
            'postgres': 'PostgreSQL',
            'mongodb': 'MongoDB',
            'mongo': 'MongoDB',
        }
    
    def normalize(self, skills: List[str]) -> List[str]:
        """ìŠ¤í‚¬ ì •ê·œí™”"""
        if not isinstance(skills, list):
            skills = [skills] if skills else []
        
        normalized = []
        for skill in skills:
            if not skill:
                continue
            
            cleaned = skill.strip().lower()
            mapped_skill = self.skill_mappings.get(cleaned, skill.strip())
            
            if len(mapped_skill) > 1 and mapped_skill not in normalized:
                normalized.append(mapped_skill)
        
        return normalized

class SalaryNormalizer:
    def normalize(self, salary_range: Dict[str, Any]) -> Dict[str, Any]:
        """ê¸‰ì—¬ ì •ê·œí™”"""
        if not isinstance(salary_range, dict):
            return {'min': 0, 'max': 0, 'negotiable': True}
        
        min_val = self._parse_salary(salary_range.get('min', 0))
        max_val = self._parse_salary(salary_range.get('max', 0))
        negotiable = salary_range.get('negotiable', True)
        
        # ë²”ìœ„ ê²€ì¦
        if min_val > max_val and max_val > 0:
            min_val, max_val = max_val, min_val
        
        # í˜„ì‹¤ì ì´ì§€ ì•Šì€ ê°’ í•„í„°ë§
        if min_val > 200000000:  # 2ì–µ ì´ˆê³¼
            min_val = 0
        if max_val > 200000000:
            max_val = 0
        
        return {
            'min': max(0, min_val),
            'max': max(0, max_val),
            'negotiable': bool(negotiable)
        }
    
    def _parse_salary(self, value) -> int:
        """ê¸‰ì—¬ ê°’ íŒŒì‹±"""
        if isinstance(value, (int, float)):
            return int(value)
        
        if not value:
            return 0
        
        # ë¬¸ìì—´ì—ì„œ ìˆ«ì ì¶”ì¶œ
        numbers = re.findall(r'\d+', str(value))
        return int(numbers[0]) if numbers else 0

# ===========================================
# web-crawling/main.py
# ===========================================
import asyncio
import argparse
import json
from typing import Dict, List, Any
from crawlers.saramin_crawler import SaraminCrawler
from processors.data_normalizer import DataNormalizer
from database.mongo_client import mongo_client
from utils.logger import setup_logger

logger = setup_logger()

class CrawlingManager:
    def __init__(self):
        self.crawlers = {
            'saramin': SaraminCrawler(),
            # ë‹¤ë¥¸ í¬ë¡¤ëŸ¬ë“¤ ì¶”ê°€ ì˜ˆì •
        }
        self.normalizer = DataNormalizer()
    
    async def crawl_all(self, options: Dict[str, Any]) -> Dict[str, Any]:
        """ëª¨ë“  ì‚¬ì´íŠ¸ í¬ë¡¤ë§"""
        logger.info("ğŸš€ í†µí•© í¬ë¡¤ë§ ì‹œì‘...")
        
        results = {
            'sites': {},
            'total': {
                'crawled': 0,
                'processed': 0,
                'saved': 0,
                'errors': 0
            }
        }
        
        sites = options.get('sites', ['saramin'])
        
        for site_name in sites:
            if site_name not in self.crawlers:
                logger.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‚¬ì´íŠ¸: {site_name}")
                continue
            
            try:
                logger.info(f"ğŸ“¡ {site_name} í¬ë¡¤ë§ ì‹œì‘...")
                
                crawler = self.crawlers[site_name]
                raw_jobs = await crawler.crawl(options)
                
                processed_jobs = []
                for raw_job in raw_jobs:
                    try:
                        normalized_job = await self.normalizer.normalize(raw_job)
                        if normalized_job.get('quality_score', 0) >= 0.5:
                            processed_jobs.append(normalized_job)
                    except Exception as e:
                        logger.warning(f"ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        results['total']['errors'] += 1
                
                # MongoDBì— ì €ì¥
                saved_count = await self.save_jobs(processed_jobs)
                
                site_result = {
                    'crawled': len(raw_jobs),
                    'processed': len(processed_jobs),
                    'saved': saved_count
                }
                
                results['sites'][site_name] = site_result
                results['total']['crawled'] += len(raw_jobs)
                results['total']['processed'] += len(processed_jobs)
                results['total']['saved'] += saved_count
                
                logger.info(f"âœ… {site_name}: {saved_count}ê°œ ì €ì¥ ì™„ë£Œ")
                
            except Exception as e:
                logger.error(f"âŒ {site_name} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                results['sites'][site_name] = {'error': str(e)}
                results['total']['errors'] += 1
        
        logger.info(f"ğŸ‰ í†µí•© í¬ë¡¤ë§ ì™„ë£Œ! ì´ {results['total']['saved']}ê°œ ì €ì¥")
        return results
    
    async def save_jobs(self, jobs: List[Dict[str, Any]]) -> int:
        """ì±„ìš©ê³µê³  MongoDBì— ì €ì¥"""
        if not jobs:
            return 0
        
        try:
            collection = mongo_client.get_collection('job_postings')
            saved_count = 0
            
            for job in jobs:
                # Upsert (ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸, ì—†ìœ¼ë©´ ì‚½ì…)
                result = collection.update_one(
                    {'id': job['id']},
                    {'$set': job},
                    upsert=True
                )
                
                if result.upserted_id or result.modified_count > 0:
                    saved_count += 1
            
            return saved_count
            
        except Exception as e:
            logger.error(f"MongoDB ì €ì¥ ì‹¤íŒ¨: {e}")
            return 0
    
    async def get_system_health(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸"""
        try:
            collection = mongo_client.get_collection('job_postings')
            
            # ê¸°ë³¸ í†µê³„
            total_jobs = collection.count_documents({})
            recent_jobs = collection.count_documents({
                'scraped_at': {'$gte': time.time() - 86400}  # ìµœê·¼ 24ì‹œê°„
            })
            
            # í’ˆì§ˆ í†µê³„
            quality_pipeline = [
                {
                    '$group': {
                        '_id': None,
                        'avg_quality': {'$avg': '$quality_score'},
                        'low_quality_count': {
                            '$sum': {
                                '$cond': [{'$lt': ['$quality_score', 0.5]}, 1, 0]
                            }
                        }
                    }
                }
            ]
            
            quality_stats = list(collection.aggregate(quality_pipeline))
            quality_data = quality_stats[0] if quality_stats else {}
            
            return {
                'database': {
                    'status': 'healthy',
                    'total_jobs': total_jobs,
                    'recent_jobs': recent_jobs
                },
                'data_quality': {
                    'average_score': round(quality_data.get('avg_quality', 0), 3),
                    'low_quality_count': quality_data.get('low_quality_count', 0)
                }
            }
            
        except Exception as e:
            logger.error(f"ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
            return {
                'database': {'status': 'error', 'error': str(e)},
                'data_quality': {'average_score': 0, 'low_quality_count': 0}
            }

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='SkillMap í¬ë¡¤ë§ ì‹œìŠ¤í…œ')
    parser.add_argument('--sites', default='saramin', help='í¬ë¡¤ë§í•  ì‚¬ì´íŠ¸')
    parser.add_argument('--keyword', default='React', help='ê²€ìƒ‰ í‚¤ì›Œë“œ')
    parser.add_argument('--category', default='IT/ê°œë°œ', help='ì§êµ° ì¹´í…Œê³ ë¦¬')
    parser.add_argument('--experience', default='ì‹ ì…', help='ê²½í—˜ ìˆ˜ì¤€')
    parser.add_argument('--max-jobs', type=int, default=50, help='ìµœëŒ€ ì±„ìš©ê³µê³  ìˆ˜')
    parser.add_argument('--output', help='ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥')
    
    args = parser.parse_args()
    
    manager = CrawlingManager()
    
    options = {
        'sites': args.sites.split(','),
        'keyword': args.keyword,
        'category': args.category,
        'experience_level': args.experience,
        'max_jobs': args.max_jobs
    }
    
    try:
        results = await manager.crawl_all(options)
        
        print(f"\nğŸ“Š í¬ë¡¤ë§ ê²°ê³¼:")
        print(f"  ì´ í¬ë¡¤ë§: {results['total']['crawled']}ê°œ")
        print(f"  ì²˜ë¦¬ì™„ë£Œ: {results['total']['processed']}ê°œ")
        print(f"  ì €ì¥ì™„ë£Œ: {results['total']['saved']}ê°œ")
        print(f"  ì˜¤ë¥˜ë°œìƒ: {results['total']['errors']}ê°œ")
        
        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ ê²°ê³¼ê°€ {args.output}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
    finally:
        mongo_client.close()

if __name__ == '__main__':
    import time
    asyncio.run(main())